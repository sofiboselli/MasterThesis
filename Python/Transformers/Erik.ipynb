{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from network import model,model2\n",
    "from audtorch.metrics import PearsonR\n",
    "from eriksdataset import EEG_Dataset\n",
    "from torchmetrics.wrappers import Running\n",
    "from torchmetrics.aggregation import MeanMetric\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "from pytorch_lightning.callbacks import EarlyStopping,ModelCheckpoint,ModelSummary\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "import os\n",
    "from diffnet import EEG_Conformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findfiles(search_dir, prefix):\n",
    "    matching_files = []\n",
    "\n",
    "    for dirpath, _, filenames in os.walk(search_dir):\n",
    "        for filename in filenames:\n",
    "            if filename.startswith(prefix):\n",
    "                full_path = os.path.join(dirpath, filename)\n",
    "                matching_files.append(full_path)\n",
    "\n",
    "    return matching_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pearsonr(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self,P,O):\n",
    "\n",
    "        (n, t) = O.shape      # n traces of t samples\n",
    "        (n_bis, m) = P.shape  # n predictions for each of m candidates\n",
    "\n",
    "        DO = O - (torch.einsum(\"nt->t\", O) / n) # compute O - mean(O)\n",
    "        DP = P - (torch.einsum(\"nm->m\", P) / n) # compute P - mean(P)\n",
    "\n",
    "        cov = torch.einsum(\"nm,nt->mt\", DP, DO)\n",
    "\n",
    "        varP = torch.einsum(\"nm,nm->m\", DP, DP)\n",
    "        varO = torch.einsum(\"nt,nt->t\", DO, DO)\n",
    "        tmp = torch.einsum(\"m,t->mt\", varP, varO)\n",
    "\n",
    "        return cov / torch.sqrt(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLoss(torch.nn.Module):\n",
    "    def __init__(self,mean,lin,scale,temp):\n",
    "        super().__init__()\n",
    "        self.mean = mean\n",
    "        self.lin = lin\n",
    "        self.scale = scale\n",
    "        self.pear = Pearsonr()\n",
    "        self.temp = temp\n",
    "    \n",
    "    def forward(self,eemb,aamb):\n",
    "        if self.mean:\n",
    "            aamb = torch.mean(aamb,dim=1,keepdim=True)\n",
    "            eemb = torch.mean(eemb,dim=1,keepdim=True)\n",
    "\n",
    "\n",
    "        if self.scale:\n",
    "            eemb =(eemb - eemb.mean(dim=1,keepdim=True))/eemb.std(dim=1,keepdim=True)\n",
    "            aamb =(aamb - aamb.mean(dim=1,keepdim=True))/aamb.std(dim=1,keepdim=True)\n",
    "\n",
    "        \n",
    "        #eemb = eemb/eemb.norm(2,dim=1,keepdim=True)\n",
    "        #aamb = aamb/aamb.norm(2,dim=1,keepdim=True)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        logits = (self.pear(eemb,aamb))*torch.exp(self.temp)\n",
    "\n",
    "\n",
    "        t= torch.arange(len(eemb[0]),device=eemb.device)\n",
    "\n",
    "\n",
    "        l1 = torch.nn.functional.cross_entropy(logits,t)\n",
    "    \n",
    "        return l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encode(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #self.encoder = model(nblock=2,lin_out=8,kernel_size=3,transformer_hidden=32,n_layers=2,n_heads=2,dff=16,filter_size=16,nconvs=2,times=320)\n",
    "        #self.encoder = model2(transformer_hidden=32,n_heads=4,dff=128)\n",
    "        self.encoder = EEG_Conformer()\n",
    "        self.temp = torch.nn.Parameter(torch.log(torch.ones(1)*5))\n",
    "        self.loss_fn = CLoss(lin=False,scale=True,mean=False,temp= self.temp)\n",
    "        self.running_loss = Running(MeanMetric(), window=32)\n",
    "        self.running_loss2 = Running(MeanMetric(),window=32)\n",
    "        self.running_loss1 = Running(MeanMetric(), window=32)\n",
    "        self.running_loss22 = Running(MeanMetric(),window=32)\n",
    "        self.los = PearsonR()\n",
    "        #self.accfn = PearsonR(reduction=None)\n",
    "        self.lr = 3e-5\n",
    "    \n",
    "\n",
    "    def forward(self, eeg):\n",
    "        # in lightning, forward defines the prediction/inference actions\n",
    "        envelope = self.encoder(eeg)\n",
    "        return envelope\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop. It is independent of forward\n",
    "        eeg,att,_ = batch\n",
    "\n",
    "        #eeg = eeg.unsqueeze(1)\n",
    "\n",
    "        eeg_emb = self(eeg)\n",
    "\n",
    "        #loss1 = self.loss_fn(eeg_emb.T,att.T)\n",
    "        loss2 = -self.los(eeg_emb,att)\n",
    "        #loss = loss1+loss2\n",
    "\n",
    "\n",
    "        #self.running_loss(loss1)\n",
    "        self.running_loss1(loss2)\n",
    "        #self.log('train_loss', self.running_loss.compute(), on_step=True, prog_bar=True)\n",
    "        self.log('train_pearson_loss', self.running_loss1.compute(), on_step=True, prog_bar=True)\n",
    "        #self.log('train_accuracy',)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "        return loss2\n",
    "\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop. It is independent of forward\n",
    "        eeg,att,_ = batch\n",
    "\n",
    "        #eeg = eeg.unsqueeze(1)\n",
    "        eeg_emb = self(eeg)\n",
    "\n",
    "        #loss1 = self.loss_fn(eeg_emb.T,att.T)\n",
    "        loss2 = -self.los(eeg_emb,att)\n",
    "        #loss = loss1+loss2\n",
    "\n",
    "        #self.running_loss2(loss1)\n",
    "        self.running_loss22(loss2)\n",
    "        #self.log('val_loss', self.running_loss2.compute(), on_step=True, prog_bar=True)\n",
    "        self.log('pearson_loss', self.running_loss22.compute(), on_step=True, prog_bar=True)\n",
    "\n",
    "        return loss2\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,800)\n",
    "        return [optimizer],[scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = findfiles(\"EriksholmFiles/\",\"on\")\n",
    "data += findfiles(\"EriksholmFiles/\",\"off\")\n",
    "\n",
    "trainfiles,valfiles = train_test_split(data,test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_dataset = EEG_Dataset(files=trainfiles,seconds=5,overlap=0.75)\n",
    "val_dataset = EEG_Dataset(files=valfiles,seconds=5,overlap=0.75)\n",
    "\n",
    "tr_loader = DataLoader(tr_dataset,batch_size=32,num_workers=3,persistent_workers=True,shuffle=True)\n",
    "val_loader = DataLoader(val_dataset,batch_size=32,num_workers=3,persistent_workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint(monitor='val_loss',dirpath='./EriksholmFiles/',filename='pretrained_weights',auto_insert_metric_name=False)\n",
    "early_stopping = EarlyStopping('val_loss',patience=10)\n",
    "ms = ModelSummary(max_depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\gauta\\anaconda3\\envs\\keras-jax\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:653: Checkpoint directory C:\\Users\\gauta\\Thesis\\PyTorch\\EriksholmFiles exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name                       | Type            | Params\n",
      "----------------------------------------------------------------\n",
      "0  | encoder                    | EEG_Conformer   | 512 K \n",
      "1  | encoder.confblock          | ModuleList      | 512 K \n",
      "2  | encoder.confblock.0        | Conformer_block | 102 K \n",
      "3  | encoder.confblock.1        | Conformer_block | 102 K \n",
      "4  | encoder.confblock.2        | Conformer_block | 102 K \n",
      "5  | encoder.confblock.3        | Conformer_block | 102 K \n",
      "6  | encoder.confblock.4        | Conformer_block | 102 K \n",
      "7  | encoder.lin                | Linear          | 65    \n",
      "8  | loss_fn                    | CLoss           | 1     \n",
      "9  | loss_fn.pear               | Pearsonr        | 0     \n",
      "10 | running_loss               | Running         | 0     \n",
      "11 | running_loss.base_metric   | MeanMetric      | 0     \n",
      "12 | running_loss2              | Running         | 0     \n",
      "13 | running_loss2.base_metric  | MeanMetric      | 0     \n",
      "14 | running_loss1              | Running         | 0     \n",
      "15 | running_loss1.base_metric  | MeanMetric      | 0     \n",
      "16 | running_loss22             | Running         | 0     \n",
      "17 | running_loss22.base_metric | MeanMetric      | 0     \n",
      "----------------------------------------------------------------\n",
      "512 K     Trainable params\n",
      "0         Non-trainable params\n",
      "512 K     Total params\n",
      "2.050     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf5a49d4dfd54214865ef6381dce6a96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m enc \u001b[38;5;241m=\u001b[39m Encode()\n\u001b[0;32m      3\u001b[0m trainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m,accelerator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpu\u001b[39m\u001b[38;5;124m'\u001b[39m,gradient_clip_val\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m,callbacks\u001b[38;5;241m=\u001b[39m[checkpoint_callback,ms])\n\u001b[1;32m----> 4\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43menc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtr_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\gauta\\anaconda3\\envs\\keras-jax\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:544\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    542\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[0;32m    543\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 544\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    545\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[0;32m    546\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\gauta\\anaconda3\\envs\\keras-jax\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trainer_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[0;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[1;32mc:\\Users\\gauta\\anaconda3\\envs\\keras-jax\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:580\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    574\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[0;32m    575\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[0;32m    576\u001b[0m     ckpt_path,\n\u001b[0;32m    577\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    578\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    579\u001b[0m )\n\u001b[1;32m--> 580\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[0;32m    583\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\gauta\\anaconda3\\envs\\keras-jax\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:987\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m    982\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[0;32m    984\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    985\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[0;32m    986\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m--> 987\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    989\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    990\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[0;32m    991\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    992\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\gauta\\anaconda3\\envs\\keras-jax\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1031\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1029\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[0;32m   1030\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[1;32m-> 1031\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_sanity_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1032\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m   1033\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mrun()\n",
      "File \u001b[1;32mc:\\Users\\gauta\\anaconda3\\envs\\keras-jax\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1060\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1057\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_start\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1059\u001b[0m \u001b[38;5;66;03m# run eval step\u001b[39;00m\n\u001b[1;32m-> 1060\u001b[0m \u001b[43mval_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1062\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1064\u001b[0m \u001b[38;5;66;03m# reset logger connector\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\gauta\\anaconda3\\envs\\keras-jax\\lib\\site-packages\\pytorch_lightning\\loops\\utilities.py:182\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    180\u001b[0m     context_manager \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mno_grad\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[1;32m--> 182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loop_run(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\gauta\\anaconda3\\envs\\keras-jax\\lib\\site-packages\\pytorch_lightning\\loops\\evaluation_loop.py:135\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mis_last_batch \u001b[38;5;241m=\u001b[39m data_fetcher\u001b[38;5;241m.\u001b[39mdone\n\u001b[0;32m    134\u001b[0m     \u001b[38;5;66;03m# run step hooks\u001b[39;00m\n\u001b[1;32m--> 135\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;66;03m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\gauta\\anaconda3\\envs\\keras-jax\\lib\\site-packages\\pytorch_lightning\\loops\\evaluation_loop.py:396\u001b[0m, in \u001b[0;36m_EvaluationLoop._evaluation_step\u001b[1;34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001b[0m\n\u001b[0;32m    390\u001b[0m hook_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_step\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mtesting \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    391\u001b[0m step_args \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    392\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_step_args_from_hook_kwargs(hook_kwargs, hook_name)\n\u001b[0;32m    393\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_dataloader_iter\n\u001b[0;32m    394\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m (dataloader_iter,)\n\u001b[0;32m    395\u001b[0m )\n\u001b[1;32m--> 396\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstep_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    398\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_processed()\n\u001b[0;32m    400\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m using_dataloader_iter:\n\u001b[0;32m    401\u001b[0m     \u001b[38;5;66;03m# update the hook kwargs now that the step method might have consumed the iterator\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\gauta\\anaconda3\\envs\\keras-jax\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:309\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[1;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[0;32m    306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 309\u001b[0m     output \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    311\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[0;32m    312\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[1;32mc:\\Users\\gauta\\anaconda3\\envs\\keras-jax\\lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:412\u001b[0m, in \u001b[0;36mStrategy.validation_step\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    410\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module:\n\u001b[0;32m    411\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_redirection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 412\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mvalidation_step(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[1;32mIn[8], line 56\u001b[0m, in \u001b[0;36mEncode.validation_step\u001b[1;34m(self, batch, batch_idx)\u001b[0m\n\u001b[0;32m     53\u001b[0m eeg_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(eeg)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m#loss1 = self.loss_fn(eeg_emb.T,att.T)\u001b[39;00m\n\u001b[1;32m---> 56\u001b[0m loss2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlos\u001b[49m\u001b[43m(\u001b[49m\u001b[43meeg_emb\u001b[49m\u001b[43m,\u001b[49m\u001b[43matt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m#loss = loss1+loss2\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m#self.running_loss2(loss1)\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_loss22(loss2)\n",
      "File \u001b[1;32mc:\\Users\\gauta\\anaconda3\\envs\\keras-jax\\lib\\site-packages\\audtorch\\metrics\\metrics.py:55\u001b[0m, in \u001b[0;36mPearsonR.__call__\u001b[1;34m(self, x, y)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, y):\n\u001b[1;32m---> 55\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpearsonr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     57\u001b[0m         r \u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mmean()\n",
      "File \u001b[1;32mc:\\Users\\gauta\\anaconda3\\envs\\keras-jax\\lib\\site-packages\\audtorch\\metrics\\functional.py:67\u001b[0m, in \u001b[0;36mpearsonr\u001b[1;34m(x, y, batch_first)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpearsonr\u001b[39m(\n\u001b[0;32m      4\u001b[0m         x,\n\u001b[0;32m      5\u001b[0m         y,\n\u001b[0;32m      6\u001b[0m         batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m      7\u001b[0m ):\n\u001b[0;32m      8\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Computes Pearson Correlation Coefficient across rows.\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \n\u001b[0;32m     10\u001b[0m \u001b[38;5;124;03m    Pearson Correlation Coefficient (also known as Linear Correlation\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     65\u001b[0m \n\u001b[0;32m     66\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[1;32m---> 67\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m y\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m batch_first:\n\u001b[0;32m     70\u001b[0m         dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "enc = Encode()\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=300,accelerator='gpu',gradient_clip_val=1.0,callbacks=[checkpoint_callback,ms])\n",
    "trainer.fit(enc, tr_loader,val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gauta\\anaconda3\\envs\\keras-jax\\lib\\site-packages\\torch\\nn\\modules\\lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "mod = Encode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Encode:\n\tMissing key(s) in state_dict: \"encoder.Enc_Pre.CNN.conv_layers.3.weight\", \"encoder.Enc_Pre.CNN.conv_layers.3.bias\", \"encoder.Enc_Pre.CNN.conv_layers.4.weight\", \"encoder.Enc_Pre.CNN.conv_layers.4.bias\", \"encoder.Enc_Pre.CNN.norm.3.weight\", \"encoder.Enc_Pre.CNN.norm.3.bias\", \"encoder.Enc_Pre.CNN.norm.4.weight\", \"encoder.Enc_Pre.CNN.norm.4.bias\", \"encoder.Enc_Pre.Convolutions.0.conv_layers.3.weight\", \"encoder.Enc_Pre.Convolutions.0.conv_layers.3.bias\", \"encoder.Enc_Pre.Convolutions.0.conv_layers.4.weight\", \"encoder.Enc_Pre.Convolutions.0.conv_layers.4.bias\", \"encoder.Enc_Pre.Convolutions.0.norm.3.weight\", \"encoder.Enc_Pre.Convolutions.0.norm.3.bias\", \"encoder.Enc_Pre.Convolutions.0.norm.4.weight\", \"encoder.Enc_Pre.Convolutions.0.norm.4.bias\", \"encoder.Enc_Pre.Convolutions.1.conv_layers.3.weight\", \"encoder.Enc_Pre.Convolutions.1.conv_layers.3.bias\", \"encoder.Enc_Pre.Convolutions.1.conv_layers.4.weight\", \"encoder.Enc_Pre.Convolutions.1.conv_layers.4.bias\", \"encoder.Enc_Pre.Convolutions.1.norm.3.weight\", \"encoder.Enc_Pre.Convolutions.1.norm.3.bias\", \"encoder.Enc_Pre.Convolutions.1.norm.4.weight\", \"encoder.Enc_Pre.Convolutions.1.norm.4.bias\", \"encoder.Enc_Pre.Convolutions.2.conv_layers.3.weight\", \"encoder.Enc_Pre.Convolutions.2.conv_layers.3.bias\", \"encoder.Enc_Pre.Convolutions.2.conv_layers.4.weight\", \"encoder.Enc_Pre.Convolutions.2.conv_layers.4.bias\", \"encoder.Enc_Pre.Convolutions.2.norm.3.weight\", \"encoder.Enc_Pre.Convolutions.2.norm.3.bias\", \"encoder.Enc_Pre.Convolutions.2.norm.4.weight\", \"encoder.Enc_Pre.Convolutions.2.norm.4.bias\", \"encoder.Enc_Pre.Convolutions.3.conv_layers.3.weight\", \"encoder.Enc_Pre.Convolutions.3.conv_layers.3.bias\", \"encoder.Enc_Pre.Convolutions.3.conv_layers.4.weight\", \"encoder.Enc_Pre.Convolutions.3.conv_layers.4.bias\", \"encoder.Enc_Pre.Convolutions.3.norm.3.weight\", \"encoder.Enc_Pre.Convolutions.3.norm.3.bias\", \"encoder.Enc_Pre.Convolutions.3.norm.4.weight\", \"encoder.Enc_Pre.Convolutions.3.norm.4.bias\", \"encoder.Enc_Pre.Convolutions.4.conv_layers.3.weight\", \"encoder.Enc_Pre.Convolutions.4.conv_layers.3.bias\", \"encoder.Enc_Pre.Convolutions.4.conv_layers.4.weight\", \"encoder.Enc_Pre.Convolutions.4.conv_layers.4.bias\", \"encoder.Enc_Pre.Convolutions.4.norm.3.weight\", \"encoder.Enc_Pre.Convolutions.4.norm.3.bias\", \"encoder.Enc_Pre.Convolutions.4.norm.4.weight\", \"encoder.Enc_Pre.Convolutions.4.norm.4.bias\". \n\tsize mismatch for encoder.Encoder.pos.inv_freq: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for encoder.Encoder.encoder_layers.self_attn.in_proj_weight: copying a param with shape torch.Size([384, 128]) from checkpoint, the shape in current model is torch.Size([768, 256]).\n\tsize mismatch for encoder.Encoder.encoder_layers.self_attn.in_proj_bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.Encoder.encoder_layers.self_attn.out_proj.weight: copying a param with shape torch.Size([128, 128]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for encoder.Encoder.encoder_layers.self_attn.out_proj.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.Encoder.encoder_layers.linear1.weight: copying a param with shape torch.Size([512, 128]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for encoder.Encoder.encoder_layers.linear1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for encoder.Encoder.encoder_layers.linear2.weight: copying a param with shape torch.Size([128, 512]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for encoder.Encoder.encoder_layers.linear2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.Encoder.encoder_layers.norm1.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.Encoder.encoder_layers.norm1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.Encoder.encoder_layers.norm2.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.Encoder.encoder_layers.norm2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.Encoder.Encoder.layers.0.self_attn.in_proj_weight: copying a param with shape torch.Size([384, 128]) from checkpoint, the shape in current model is torch.Size([768, 256]).\n\tsize mismatch for encoder.Encoder.Encoder.layers.0.self_attn.in_proj_bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.Encoder.Encoder.layers.0.self_attn.out_proj.weight: copying a param with shape torch.Size([128, 128]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for encoder.Encoder.Encoder.layers.0.self_attn.out_proj.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.Encoder.Encoder.layers.0.linear1.weight: copying a param with shape torch.Size([512, 128]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for encoder.Encoder.Encoder.layers.0.linear1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for encoder.Encoder.Encoder.layers.0.linear2.weight: copying a param with shape torch.Size([128, 512]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for encoder.Encoder.Encoder.layers.0.linear2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.Encoder.Encoder.layers.0.norm1.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.Encoder.Encoder.layers.0.norm1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.Encoder.Encoder.layers.0.norm2.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.Encoder.Encoder.layers.0.norm2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.Encoder.Encoder.layers.1.self_attn.in_proj_weight: copying a param with shape torch.Size([384, 128]) from checkpoint, the shape in current model is torch.Size([768, 256]).\n\tsize mismatch for encoder.Encoder.Encoder.layers.1.self_attn.in_proj_bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.Encoder.Encoder.layers.1.self_attn.out_proj.weight: copying a param with shape torch.Size([128, 128]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for encoder.Encoder.Encoder.layers.1.self_attn.out_proj.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.Encoder.Encoder.layers.1.linear1.weight: copying a param with shape torch.Size([512, 128]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for encoder.Encoder.Encoder.layers.1.linear1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for encoder.Encoder.Encoder.layers.1.linear2.weight: copying a param with shape torch.Size([128, 512]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for encoder.Encoder.Encoder.layers.1.linear2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.Encoder.Encoder.layers.1.norm1.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.Encoder.Encoder.layers.1.norm1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.Encoder.Encoder.layers.1.norm2.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.Encoder.Encoder.layers.1.norm2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.Encoder.Encoder.layers.2.self_attn.in_proj_weight: copying a param with shape torch.Size([384, 128]) from checkpoint, the shape in current model is torch.Size([768, 256]).\n\tsize mismatch for encoder.Encoder.Encoder.layers.2.self_attn.in_proj_bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.Encoder.Encoder.layers.2.self_attn.out_proj.weight: copying a param with shape torch.Size([128, 128]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for encoder.Encoder.Encoder.layers.2.self_attn.out_proj.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.Encoder.Encoder.layers.2.linear1.weight: copying a param with shape torch.Size([512, 128]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for encoder.Encoder.Encoder.layers.2.linear1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for encoder.Encoder.Encoder.layers.2.linear2.weight: copying a param with shape torch.Size([128, 512]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for encoder.Encoder.Encoder.layers.2.linear2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.Encoder.Encoder.layers.2.norm1.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.Encoder.Encoder.layers.2.norm1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.Encoder.Encoder.layers.2.norm2.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.Encoder.Encoder.layers.2.norm2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.Encoder.Encoder.layers.3.self_attn.in_proj_weight: copying a param with shape torch.Size([384, 128]) from checkpoint, the shape in current model is torch.Size([768, 256]).\n\tsize mismatch for encoder.Encoder.Encoder.layers.3.self_attn.in_proj_bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.Encoder.Encoder.layers.3.self_attn.out_proj.weight: copying a param with shape torch.Size([128, 128]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for encoder.Encoder.Encoder.layers.3.self_attn.out_proj.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.Encoder.Encoder.layers.3.linear1.weight: copying a param with shape torch.Size([512, 128]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for encoder.Encoder.Encoder.layers.3.linear1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for encoder.Encoder.Encoder.layers.3.linear2.weight: copying a param with shape torch.Size([128, 512]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for encoder.Encoder.Encoder.layers.3.linear2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.Encoder.Encoder.layers.3.norm1.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.Encoder.Encoder.layers.3.norm1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.Encoder.Encoder.layers.3.norm2.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.Encoder.Encoder.layers.3.norm2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.Encoder.Encoder.norm.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.Encoder.Encoder.norm.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.int_conv.weight: copying a param with shape torch.Size([128, 64, 1]) from checkpoint, the shape in current model is torch.Size([256, 64, 1]).\n\tsize mismatch for encoder.int_conv.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m mod \u001b[38;5;241m=\u001b[39m \u001b[43mEncode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_from_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/DTUFiles/pretrained_weights.ckpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\gauta\\anaconda3\\envs\\keras-jax\\lib\\site-packages\\pytorch_lightning\\utilities\\model_helpers.py:125\u001b[0m, in \u001b[0;36m_restricted_classmethod_impl.__get__.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m instance \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_scripting:\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    122\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe classmethod `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmethod\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` cannot be called on an instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    123\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Please call it on the class type and make sure the return value is used.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    124\u001b[0m     )\n\u001b[1;32m--> 125\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmethod(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\gauta\\anaconda3\\envs\\keras-jax\\lib\\site-packages\\pytorch_lightning\\core\\module.py:1581\u001b[0m, in \u001b[0;36mLightningModule.load_from_checkpoint\u001b[1;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[0;32m   1492\u001b[0m \u001b[38;5;129m@_restricted_classmethod\u001b[39m\n\u001b[0;32m   1493\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_from_checkpoint\u001b[39m(\n\u001b[0;32m   1494\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1499\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m   1500\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Primary way of loading a model from a checkpoint. When Lightning saves a checkpoint it stores the arguments\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;124;03m    passed to ``__init__``  in the checkpoint under ``\"hyper_parameters\"``.\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1579\u001b[0m \n\u001b[0;32m   1580\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1581\u001b[0m     loaded \u001b[38;5;241m=\u001b[39m _load_from_checkpoint(\n\u001b[0;32m   1582\u001b[0m         \u001b[38;5;28mcls\u001b[39m,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m   1583\u001b[0m         checkpoint_path,\n\u001b[0;32m   1584\u001b[0m         map_location,\n\u001b[0;32m   1585\u001b[0m         hparams_file,\n\u001b[0;32m   1586\u001b[0m         strict,\n\u001b[0;32m   1587\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1588\u001b[0m     )\n\u001b[0;32m   1589\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(Self, loaded)\n",
      "File \u001b[1;32mc:\\Users\\gauta\\anaconda3\\envs\\keras-jax\\lib\\site-packages\\pytorch_lightning\\core\\saving.py:91\u001b[0m, in \u001b[0;36m_load_from_checkpoint\u001b[1;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _load_state(\u001b[38;5;28mcls\u001b[39m, checkpoint, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, pl\u001b[38;5;241m.\u001b[39mLightningModule):\n\u001b[1;32m---> 91\u001b[0m     model \u001b[38;5;241m=\u001b[39m _load_state(\u001b[38;5;28mcls\u001b[39m, checkpoint, strict\u001b[38;5;241m=\u001b[39mstrict, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     92\u001b[0m     state_dict \u001b[38;5;241m=\u001b[39m checkpoint[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstate_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m state_dict:\n",
      "File \u001b[1;32mc:\\Users\\gauta\\anaconda3\\envs\\keras-jax\\lib\\site-packages\\pytorch_lightning\\core\\saving.py:180\u001b[0m, in \u001b[0;36m_load_state\u001b[1;34m(cls, checkpoint, strict, **cls_kwargs_new)\u001b[0m\n\u001b[0;32m    177\u001b[0m     obj\u001b[38;5;241m.\u001b[39mon_load_checkpoint(checkpoint)\n\u001b[0;32m    179\u001b[0m \u001b[38;5;66;03m# load the state_dict on the model automatically\u001b[39;00m\n\u001b[1;32m--> 180\u001b[0m keys \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstate_dict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m strict:\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m keys\u001b[38;5;241m.\u001b[39mmissing_keys:\n",
      "File \u001b[1;32mc:\\Users\\gauta\\anaconda3\\envs\\keras-jax\\lib\\site-packages\\torch\\nn\\modules\\module.py:2152\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[0;32m   2147\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   2148\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2149\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[0;32m   2151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2152\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2153\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[0;32m   2154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Encode:\n\tMissing key(s) in state_dict: \"encoder.Enc_Pre.CNN.conv_layers.3.weight\", \"encoder.Enc_Pre.CNN.conv_layers.3.bias\", \"encoder.Enc_Pre.CNN.conv_layers.4.weight\", \"encoder.Enc_Pre.CNN.conv_layers.4.bias\", \"encoder.Enc_Pre.CNN.norm.3.weight\", \"encoder.Enc_Pre.CNN.norm.3.bias\", \"encoder.Enc_Pre.CNN.norm.4.weight\", \"encoder.Enc_Pre.CNN.norm.4.bias\", \"encoder.Enc_Pre.Convolutions.0.conv_layers.3.weight\", \"encoder.Enc_Pre.Convolutions.0.conv_layers.3.bias\", \"encoder.Enc_Pre.Convolutions.0.conv_layers.4.weight\", \"encoder.Enc_Pre.Convolutions.0.conv_layers.4.bias\", \"encoder.Enc_Pre.Convolutions.0.norm.3.weight\", \"encoder.Enc_Pre.Convolutions.0.norm.3.bias\", \"encoder.Enc_Pre.Convolutions.0.norm.4.weight\", \"encoder.Enc_Pre.Convolutions.0.norm.4.bias\", \"encoder.Enc_Pre.Convolutions.1.conv_layers.3.weight\", \"encoder.Enc_Pre.Convolutions.1.conv_layers.3.bias\", \"encoder.Enc_Pre.Convolutions.1.conv_layers.4.weight\", \"encoder.Enc_Pre.Convolutions.1.conv_layers.4.bias\", \"encoder.Enc_Pre.Convolutions.1.norm.3.weight\", \"encoder.Enc_Pre.Convolutions.1.norm.3.bias\", \"encoder.Enc_Pre.Convolutions.1.norm.4.weight\", \"encoder.Enc_Pre.Convolutions.1.norm.4.bias\", \"encoder.Enc_Pre.Convolutions.2.conv_layers.3.weight\", \"encoder.Enc_Pre.Convolutions.2.conv_layers.3.bias\", \"encoder.Enc_Pre.Convolutions.2.conv_layers.4.weight\", \"encoder.Enc_Pre.Convolutions.2.conv_layers.4.bias\", \"encoder.Enc_Pre.Convolutions.2.norm.3.weight\", \"encoder.Enc_Pre.Convolutions.2.norm.3.bias\", \"encoder.Enc_Pre.Convolutions.2.norm.4.weight\", \"encoder.Enc_Pre.Convolutions.2.norm.4.bias\", \"encoder.Enc_Pre.Convolutions.3.conv_layers.3.weight\", \"encoder.Enc_Pre.Convolutions.3.conv_layers.3.bias\", \"encoder.Enc_Pre.Convolutions.3.conv_layers.4.weight\", \"encoder.Enc_Pre.Convolutions.3.conv_layers.4.bias\", \"encoder.Enc_Pre.Convolutions.3.norm.3.weight\", \"encoder.Enc_Pre.Convolutions.3.norm.3.bias\", \"encoder.Enc_Pre.Convolutions.3.norm.4.weight\", \"encoder.Enc_Pre.Convolutions.3.norm.4.bias\", \"encoder.Enc_Pre.Convolutions.4.conv_layers.3.weight\", \"encoder.Enc_Pre.Convolutions.4.conv_layers.3.bias\", \"encoder.Enc_Pre.Convolutions.4.conv_layers.4.weight\", \"encoder.Enc_Pre.Convolutions.4.conv_layers.4.bias\", \"encoder.Enc_Pre.Convolutions.4.norm.3.weight\", \"encoder.Enc_Pre.Convolutions.4.norm.3.bias\", \"encoder.Enc_Pre.Convolutions.4.norm.4.weight\", \"encoder.Enc_Pre.Convolutions.4.norm.4.bias\". \n\tsize mismatch for encoder.Encoder.pos.inv_freq: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for encoder.Encoder.encoder_layers.self_attn.in_proj_weight: copying a param with shape torch.Size([384, 128]) from checkpoint, the shape in current model is torch.Size([768, 256]).\n\tsize mismatch for encoder.Encoder.encoder_layers.self_attn.in_proj_bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.Encoder.encoder_layers.self_attn.out_proj.weight: copying a param with shape torch.Size([128, 128]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for encoder.Encoder.encoder_layers.self_attn.out_proj.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.Encoder.encoder_layers.linear1.weight: copying a param with shape torch.Size([512, 128]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for encoder.Encoder.encoder_layers.linear1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for encoder.Encoder.encoder_layers.linear2.weight: copying a param with shape torch.Size([128, 512]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for encoder.Encoder.encoder_layers.linear2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.Encoder.encoder_layers.norm1.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.Encoder.encoder_layers.norm1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.Encoder.encoder_layers.norm2.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.Encoder.encoder_layers.norm2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.Encoder.Encoder.layers.0.self_attn.in_proj_weight: copying a param with shape torch.Size([384, 128]) from checkpoint, the shape in current model is torch.Size([768, 256]).\n\tsize mismatch for encoder.Encoder.Encoder.layers.0.self_attn.in_proj_bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.Encoder.Encoder.layers.0.self_attn.out_proj.weight: copying a param with shape torch.Size([128, 128]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for encoder.Encoder.Encoder.layers.0.self_attn.out_proj.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.Encoder.Encoder.layers.0.linear1.weight: copying a param with shape torch.Size([512, 128]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for encoder.Encoder.Encoder.layers.0.linear1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for encoder.Encoder.Encoder.layers.0.linear2.weight: copying a param with shape torch.Size([128, 512]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for encoder.Encoder.Encoder.layers.0.linear2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.Encoder.Encoder.layers.0.norm1.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.Encoder.Encoder.layers.0.norm1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.Encoder.Encoder.layers.0.norm2.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.Encoder.Encoder.layers.0.norm2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.Encoder.Encoder.layers.1.self_attn.in_proj_weight: copying a param with shape torch.Size([384, 128]) from checkpoint, the shape in current model is torch.Size([768, 256]).\n\tsize mismatch for encoder.Encoder.Encoder.layers.1.self_attn.in_proj_bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.Encoder.Encoder.layers.1.self_attn.out_proj.weight: copying a param with shape torch.Size([128, 128]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for encoder.Encoder.Encoder.layers.1.self_attn.out_proj.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.Encoder.Encoder.layers.1.linear1.weight: copying a param with shape torch.Size([512, 128]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for encoder.Encoder.Encoder.layers.1.linear1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for encoder.Encoder.Encoder.layers.1.linear2.weight: copying a param with shape torch.Size([128, 512]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for encoder.Encoder.Encoder.layers.1.linear2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.Encoder.Encoder.layers.1.norm1.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.Encoder.Encoder.layers.1.norm1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.Encoder.Encoder.layers.1.norm2.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.Encoder.Encoder.layers.1.norm2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.Encoder.Encoder.layers.2.self_attn.in_proj_weight: copying a param with shape torch.Size([384, 128]) from checkpoint, the shape in current model is torch.Size([768, 256]).\n\tsize mismatch for encoder.Encoder.Encoder.layers.2.self_attn.in_proj_bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.Encoder.Encoder.layers.2.self_attn.out_proj.weight: copying a param with shape torch.Size([128, 128]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for encoder.Encoder.Encoder.layers.2.self_attn.out_proj.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.Encoder.Encoder.layers.2.linear1.weight: copying a param with shape torch.Size([512, 128]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for encoder.Encoder.Encoder.layers.2.linear1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for encoder.Encoder.Encoder.layers.2.linear2.weight: copying a param with shape torch.Size([128, 512]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for encoder.Encoder.Encoder.layers.2.linear2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.Encoder.Encoder.layers.2.norm1.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.Encoder.Encoder.layers.2.norm1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.Encoder.Encoder.layers.2.norm2.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.Encoder.Encoder.layers.2.norm2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.Encoder.Encoder.layers.3.self_attn.in_proj_weight: copying a param with shape torch.Size([384, 128]) from checkpoint, the shape in current model is torch.Size([768, 256]).\n\tsize mismatch for encoder.Encoder.Encoder.layers.3.self_attn.in_proj_bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.Encoder.Encoder.layers.3.self_attn.out_proj.weight: copying a param with shape torch.Size([128, 128]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for encoder.Encoder.Encoder.layers.3.self_attn.out_proj.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.Encoder.Encoder.layers.3.linear1.weight: copying a param with shape torch.Size([512, 128]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for encoder.Encoder.Encoder.layers.3.linear1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for encoder.Encoder.Encoder.layers.3.linear2.weight: copying a param with shape torch.Size([128, 512]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for encoder.Encoder.Encoder.layers.3.linear2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.Encoder.Encoder.layers.3.norm1.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.Encoder.Encoder.layers.3.norm1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.Encoder.Encoder.layers.3.norm2.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.Encoder.Encoder.layers.3.norm2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.Encoder.Encoder.norm.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.Encoder.Encoder.norm.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.int_conv.weight: copying a param with shape torch.Size([128, 64, 1]) from checkpoint, the shape in current model is torch.Size([256, 64, 1]).\n\tsize mismatch for encoder.int_conv.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256])."
     ]
    }
   ],
   "source": [
    "mod = Encode.load_from_checkpoint(\"/DTUFiles/pretrained_weights.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 21/21 [00:21<00:00,  1.01s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "test_loss = []\n",
    "test_metric = []\n",
    "test_masker = []\n",
    "test_model = enc.cpu().eval()\n",
    "acc = []\n",
    "\n",
    "test_metric_dict = {}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_data in tqdm(val_loader):\n",
    "        inputs, outputs,masker = batch_data[0].squeeze(0), batch_data[1].squeeze(0),batch_data[2].squeeze(0)\n",
    "        #inputs = inputs.unsqueeze(1)\n",
    "\n",
    "        # Forward pass\n",
    "        predictions = test_model(inputs)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = -PearsonR()(outputs, predictions).mean()\n",
    "\n",
    "        # Compute metric\n",
    "        metric = PearsonR(reduction=None)(outputs, predictions)\n",
    "        metric2 = PearsonR(reduction=None)(masker, predictions)\n",
    "        accu = torch.sum(metric>metric2)/metric.shape[0]\n",
    "\n",
    "        test_loss.append(loss.item())\n",
    "        test_metric.append(metric.mean().detach().cpu().numpy().reshape(-1, 1))\n",
    "        test_masker.append(metric2.mean().detach().cpu().numpy().reshape(-1, 1))\n",
    "        acc.append(accu.detach().numpy())\n",
    "        #test_metric_dict[name] = metric.detach().cpu().numpy().reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.62614524"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.mean(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(0.28125, dtype=float32),\n",
       " array(0.40625, dtype=float32),\n",
       " array(0.59375, dtype=float32),\n",
       " array(0.46875, dtype=float32),\n",
       " array(0.6875, dtype=float32),\n",
       " array(0.5625, dtype=float32),\n",
       " array(0.8125, dtype=float32),\n",
       " array(0.8125, dtype=float32),\n",
       " array(0.53125, dtype=float32),\n",
       " array(0.65625, dtype=float32),\n",
       " array(0.59375, dtype=float32),\n",
       " array(0.34375, dtype=float32),\n",
       " array(0.6875, dtype=float32),\n",
       " array(0.625, dtype=float32),\n",
       " array(0.59375, dtype=float32),\n",
       " array(0.375, dtype=float32),\n",
       " array(0.59375, dtype=float32),\n",
       " array(0.4375, dtype=float32),\n",
       " array(0.5, dtype=float32),\n",
       " array(0.40625, dtype=float32),\n",
       " array(0.5625, dtype=float32),\n",
       " array(0.625, dtype=float32),\n",
       " array(0.65625, dtype=float32),\n",
       " array(0.53125, dtype=float32),\n",
       " array(0.71875, dtype=float32),\n",
       " array(0.78125, dtype=float32),\n",
       " array(0.40625, dtype=float32),\n",
       " array(0.71875, dtype=float32),\n",
       " array(0.53125, dtype=float32),\n",
       " array(0.46875, dtype=float32),\n",
       " array(0.34375, dtype=float32),\n",
       " array(0.15625, dtype=float32),\n",
       " array(0.84375, dtype=float32),\n",
       " array(0.4375, dtype=float32),\n",
       " array(0.40625, dtype=float32),\n",
       " array(0.78125, dtype=float32),\n",
       " array(0.625, dtype=float32),\n",
       " array(0.65625, dtype=float32),\n",
       " array(0.5625, dtype=float32),\n",
       " array(0.5625, dtype=float32),\n",
       " array(0.71875, dtype=float32),\n",
       " array(0.6875, dtype=float32),\n",
       " array(0.9375, dtype=float32),\n",
       " array(0.6875, dtype=float32),\n",
       " array(0.75, dtype=float32),\n",
       " array(0.6875, dtype=float32),\n",
       " array(0.625, dtype=float32),\n",
       " array(0.15625, dtype=float32),\n",
       " array(0.5625, dtype=float32),\n",
       " array(0.75, dtype=float32),\n",
       " array(0.71875, dtype=float32),\n",
       " array(0.625, dtype=float32),\n",
       " array(0.46875, dtype=float32),\n",
       " array(0.9375, dtype=float32),\n",
       " array(0.84375, dtype=float32),\n",
       " array(0.6875, dtype=float32),\n",
       " array(0.5, dtype=float32),\n",
       " array(0.53125, dtype=float32),\n",
       " array(0.59375, dtype=float32),\n",
       " array(0.8125, dtype=float32),\n",
       " array(0.5625, dtype=float32),\n",
       " array(0.59375, dtype=float32),\n",
       " array(0.4375, dtype=float32),\n",
       " array(0.4375, dtype=float32),\n",
       " array(0.65625, dtype=float32),\n",
       " array(0.375, dtype=float32),\n",
       " array(0.5625, dtype=float32),\n",
       " array(0.34375, dtype=float32),\n",
       " array(0.65625, dtype=float32),\n",
       " array(0.5625, dtype=float32),\n",
       " array(0.59375, dtype=float32),\n",
       " array(0.84375, dtype=float32),\n",
       " array(0.65625, dtype=float32),\n",
       " array(0.8125, dtype=float32),\n",
       " array(0.5625, dtype=float32),\n",
       " array(0.5, dtype=float32),\n",
       " array(0.34375, dtype=float32),\n",
       " array(0.75, dtype=float32),\n",
       " array(0.5625, dtype=float32),\n",
       " array(0.4375, dtype=float32),\n",
       " array(0.46875, dtype=float32),\n",
       " array(0.375, dtype=float32),\n",
       " array(0.53125, dtype=float32),\n",
       " array(0.46875, dtype=float32),\n",
       " array(0.5, dtype=float32),\n",
       " array(0.625, dtype=float32),\n",
       " array(0.5625, dtype=float32),\n",
       " array(0.625, dtype=float32),\n",
       " array(0.625, dtype=float32),\n",
       " array(0.71875, dtype=float32),\n",
       " array(0.84375, dtype=float32),\n",
       " array(0.6875, dtype=float32),\n",
       " array(0.625, dtype=float32),\n",
       " array(0.59375, dtype=float32),\n",
       " array(0.5625, dtype=float32),\n",
       " array(0.65625, dtype=float32),\n",
       " array(0.5, dtype=float32),\n",
       " array(0.78125, dtype=float32),\n",
       " array(0.625, dtype=float32),\n",
       " array(0.375, dtype=float32),\n",
       " array(0.3125, dtype=float32),\n",
       " array(0.25, dtype=float32),\n",
       " array(0.6875, dtype=float32),\n",
       " array(0.6875, dtype=float32),\n",
       " array(0.59375, dtype=float32),\n",
       " array(0.59375, dtype=float32),\n",
       " array(0.625, dtype=float32),\n",
       " array(0.65625, dtype=float32),\n",
       " array(0.53125, dtype=float32),\n",
       " array(0.40625, dtype=float32),\n",
       " array(0.21875, dtype=float32),\n",
       " array(0.6875, dtype=float32),\n",
       " array(0.4375, dtype=float32),\n",
       " array(0.46875, dtype=float32),\n",
       " array(0.8125, dtype=float32),\n",
       " array(0.3125, dtype=float32),\n",
       " array(0.4375, dtype=float32),\n",
       " array(0.5625, dtype=float32),\n",
       " array(0.5, dtype=float32),\n",
       " array(0.65625, dtype=float32),\n",
       " array(0.59375, dtype=float32),\n",
       " array(0.34375, dtype=float32),\n",
       " array(0.5625, dtype=float32),\n",
       " array(0.53125, dtype=float32),\n",
       " array(0.59375, dtype=float32),\n",
       " array(0.5625, dtype=float32),\n",
       " array(0.5, dtype=float32),\n",
       " array(0.5, dtype=float32),\n",
       " array(0.34375, dtype=float32),\n",
       " array(0.5, dtype=float32),\n",
       " array(0.625, dtype=float32),\n",
       " array(0.59375, dtype=float32),\n",
       " array(0.71875, dtype=float32),\n",
       " array(0.65625, dtype=float32),\n",
       " array(0.6875, dtype=float32),\n",
       " array(0.75, dtype=float32),\n",
       " array(0.6875, dtype=float32),\n",
       " array(0.78125, dtype=float32),\n",
       " array(0.59375, dtype=float32),\n",
       " array(0.65625, dtype=float32),\n",
       " array(0.5625, dtype=float32),\n",
       " array(0.625, dtype=float32),\n",
       " array(0.6875, dtype=float32),\n",
       " array(0.75, dtype=float32),\n",
       " array(0.5625, dtype=float32),\n",
       " array(0.59375, dtype=float32),\n",
       " array(0.5, dtype=float32),\n",
       " array(0.5625, dtype=float32),\n",
       " array(0.4375, dtype=float32),\n",
       " array(0.6875, dtype=float32),\n",
       " array(0.65625, dtype=float32),\n",
       " array(0.71875, dtype=float32),\n",
       " array(0.625, dtype=float32),\n",
       " array(0.78125, dtype=float32),\n",
       " array(0.65625, dtype=float32),\n",
       " array(0.53125, dtype=float32),\n",
       " array(0.28125, dtype=float32),\n",
       " array(0.6875, dtype=float32),\n",
       " array(0.4375, dtype=float32),\n",
       " array(0.71875, dtype=float32),\n",
       " array(0.59375, dtype=float32),\n",
       " array(0.6875, dtype=float32),\n",
       " array(0.4375, dtype=float32),\n",
       " array(0.53125, dtype=float32),\n",
       " array(0.78125, dtype=float32),\n",
       " array(0.5625, dtype=float32),\n",
       " array(0.71875, dtype=float32),\n",
       " array(0.625, dtype=float32),\n",
       " array(0.59375, dtype=float32),\n",
       " array(0.5, dtype=float32),\n",
       " array(0.5625, dtype=float32),\n",
       " array(0.5, dtype=float32),\n",
       " array(0.53125, dtype=float32),\n",
       " array(0.53125, dtype=float32),\n",
       " array(0.6875, dtype=float32),\n",
       " array(0.71875, dtype=float32),\n",
       " array(0.75, dtype=float32),\n",
       " array(0.625, dtype=float32),\n",
       " array(0.53125, dtype=float32),\n",
       " array(0.75, dtype=float32),\n",
       " array(0.5, dtype=float32),\n",
       " array(0.78125, dtype=float32),\n",
       " array(0.6875, dtype=float32),\n",
       " array(0.5, dtype=float32),\n",
       " array(0.5, dtype=float32),\n",
       " array(0.5625, dtype=float32),\n",
       " array(0.875, dtype=float32),\n",
       " array(0.5, dtype=float32),\n",
       " array(0.6875, dtype=float32),\n",
       " array(0.6875, dtype=float32),\n",
       " array(0.34375, dtype=float32),\n",
       " array(0.375, dtype=float32),\n",
       " array(0.65625, dtype=float32),\n",
       " array(0.8125, dtype=float32),\n",
       " array(0.8125, dtype=float32),\n",
       " array(0.84375, dtype=float32),\n",
       " array(0.8125, dtype=float32),\n",
       " array(0.5625, dtype=float32),\n",
       " array(0.5625, dtype=float32),\n",
       " array(0.78125, dtype=float32),\n",
       " array(0.625, dtype=float32),\n",
       " array(0.46875, dtype=float32),\n",
       " array(0.5, dtype=float32),\n",
       " array(0.84375, dtype=float32),\n",
       " array(0.53125, dtype=float32),\n",
       " array(0.6875, dtype=float32),\n",
       " array(0.46875, dtype=float32),\n",
       " array(0.46875, dtype=float32),\n",
       " array(0.5, dtype=float32),\n",
       " array(0.78125, dtype=float32),\n",
       " array(0.71875, dtype=float32),\n",
       " array(0.5, dtype=float32),\n",
       " array(0.71875, dtype=float32),\n",
       " array(0.78125, dtype=float32),\n",
       " array(0.53125, dtype=float32),\n",
       " array(0.5625, dtype=float32),\n",
       " array(0.6875, dtype=float32),\n",
       " array(0.84375, dtype=float32),\n",
       " array(0.5625, dtype=float32),\n",
       " array(0.78125, dtype=float32),\n",
       " array(0.59375, dtype=float32),\n",
       " array(0.65625, dtype=float32),\n",
       " array(0.625, dtype=float32),\n",
       " array(0.5625, dtype=float32),\n",
       " array(0.4375, dtype=float32),\n",
       " array(0.65625, dtype=float32),\n",
       " array(0.59375, dtype=float32),\n",
       " array(0.5625, dtype=float32),\n",
       " array(0.71875, dtype=float32),\n",
       " array(0.5, dtype=float32),\n",
       " array(0.46875, dtype=float32),\n",
       " array(0.6875, dtype=float32),\n",
       " array(0.8125, dtype=float32),\n",
       " array(0.59375, dtype=float32),\n",
       " array(0.59375, dtype=float32),\n",
       " array(0.78125, dtype=float32),\n",
       " array(0.59375, dtype=float32),\n",
       " array(0.5, dtype=float32),\n",
       " array(0.6875, dtype=float32),\n",
       " array(0.6875, dtype=float32),\n",
       " array(0.53125, dtype=float32),\n",
       " array(0.4375, dtype=float32),\n",
       " array(0.59375, dtype=float32),\n",
       " array(0.65625, dtype=float32),\n",
       " array(0.59375, dtype=float32),\n",
       " array(0.53125, dtype=float32),\n",
       " array(0.4375, dtype=float32),\n",
       " array(0.59375, dtype=float32),\n",
       " array(0.34375, dtype=float32),\n",
       " array(0.71875, dtype=float32),\n",
       " array(0.6875, dtype=float32),\n",
       " array(0.46875, dtype=float32),\n",
       " array(0.6875, dtype=float32),\n",
       " array(0.59375, dtype=float32),\n",
       " array(0.5625, dtype=float32),\n",
       " array(0.6875, dtype=float32),\n",
       " array(0.8125, dtype=float32),\n",
       " array(0.8125, dtype=float32),\n",
       " array(0.75, dtype=float32)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.104037605"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(test_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.032472655"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(test_masker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.1269815012931277"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = model2()\n",
    "x = torch.rand([1,1,64,256])\n",
    "out = c(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 20, 32])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = CNN()\n",
    "x = torch.rand([1,1,64,256])\n",
    "out = c(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class out_model(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = torch.nn.ConvTranspose1d(32,32,55,stride=9)\n",
    "        self.conv2 = torch.nn.ConvTranspose1d(32,32,31,1)\n",
    "\n",
    "    \n",
    "    def forward(self,x):\n",
    "\n",
    "        out = self.conv(x)\n",
    "        out = self.conv2(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 256])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = out_model()\n",
    "x = torch.rand([1,32,20])\n",
    "out = c(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.tuner import Tuner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca78ec7b358641869e9b8fa19167b213",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.2754228703338169\n",
      "Restoring states from the checkpoint path at c:\\Users\\gauta\\Thesis\\PyTorch\\.lr_find_4950efaa-0fbd-4ca8-a567-834434e22c27.ckpt\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Can't access the shape of an uninitialized parameter or buffer. This error usually happens in `load_state_dict` when trying to load an uninitialized parameter into an initialized one. Call `forward` to initialize the parameters before accessing their attributes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[60], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m tuner \u001b[38;5;241m=\u001b[39m Tuner(trainer)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Run learning rate finder\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m lr_finder \u001b[38;5;241m=\u001b[39m \u001b[43mtuner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlr_find\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtr_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Results can be found in\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(lr_finder\u001b[38;5;241m.\u001b[39mresults)\n",
      "File \u001b[1;32mc:\\Users\\gauta\\anaconda3\\envs\\keras-jax\\lib\\site-packages\\pytorch_lightning\\tuner\\tuning.py:180\u001b[0m, in \u001b[0;36mTuner.lr_find\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, dataloaders, datamodule, method, min_lr, max_lr, num_training, mode, early_stop_threshold, update_attr, attr_name)\u001b[0m\n\u001b[0;32m    177\u001b[0m lr_finder_callback\u001b[38;5;241m.\u001b[39m_early_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer\u001b[38;5;241m.\u001b[39mcallbacks \u001b[38;5;241m=\u001b[39m [lr_finder_callback] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer\u001b[38;5;241m.\u001b[39mcallbacks\n\u001b[1;32m--> 180\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer\u001b[38;5;241m.\u001b[39mcallbacks \u001b[38;5;241m=\u001b[39m [cb \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer\u001b[38;5;241m.\u001b[39mcallbacks \u001b[38;5;28;01mif\u001b[39;00m cb \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lr_finder_callback]\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m lr_finder_callback\u001b[38;5;241m.\u001b[39moptimal_lr\n",
      "File \u001b[1;32mc:\\Users\\gauta\\anaconda3\\envs\\keras-jax\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:544\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    542\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[0;32m    543\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 544\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    545\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[0;32m    546\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\gauta\\anaconda3\\envs\\keras-jax\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trainer_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[0;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[1;32mc:\\Users\\gauta\\anaconda3\\envs\\keras-jax\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:580\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    574\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[0;32m    575\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[0;32m    576\u001b[0m     ckpt_path,\n\u001b[0;32m    577\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    578\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    579\u001b[0m )\n\u001b[1;32m--> 580\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[0;32m    583\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\gauta\\anaconda3\\envs\\keras-jax\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:967\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m    965\u001b[0m \u001b[38;5;66;03m# hook\u001b[39;00m\n\u001b[0;32m    966\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;241m==\u001b[39m TrainerFn\u001b[38;5;241m.\u001b[39mFITTING:\n\u001b[1;32m--> 967\u001b[0m     \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_callback_hooks\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mon_fit_start\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    968\u001b[0m     call\u001b[38;5;241m.\u001b[39m_call_lightning_module_hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_fit_start\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    970\u001b[0m _log_hyperparams(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\gauta\\anaconda3\\envs\\keras-jax\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:208\u001b[0m, in \u001b[0;36m_call_callback_hooks\u001b[1;34m(trainer, hook_name, monitoring_callbacks, *args, **kwargs)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(fn):\n\u001b[0;32m    207\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Callback]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcallback\u001b[38;5;241m.\u001b[39mstate_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 208\u001b[0m             fn(trainer, trainer\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pl_module:\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[0;32m    212\u001b[0m     pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[1;32mc:\\Users\\gauta\\anaconda3\\envs\\keras-jax\\lib\\site-packages\\pytorch_lightning\\callbacks\\lr_finder.py:130\u001b[0m, in \u001b[0;36mLearningRateFinder.on_fit_start\u001b[1;34m(self, trainer, pl_module)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_fit_start\u001b[39m(\u001b[38;5;28mself\u001b[39m, trainer: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpl.Trainer\u001b[39m\u001b[38;5;124m\"\u001b[39m, pl_module: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpl.LightningModule\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 130\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlr_find\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpl_module\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\gauta\\anaconda3\\envs\\keras-jax\\lib\\site-packages\\pytorch_lightning\\callbacks\\lr_finder.py:113\u001b[0m, in \u001b[0;36mLearningRateFinder.lr_find\u001b[1;34m(self, trainer, pl_module)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlr_find\u001b[39m(\u001b[38;5;28mself\u001b[39m, trainer: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpl.Trainer\u001b[39m\u001b[38;5;124m\"\u001b[39m, pl_module: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpl.LightningModule\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[1;32m--> 113\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimal_lr \u001b[38;5;241m=\u001b[39m \u001b[43m_lr_find\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    115\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpl_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmin_lr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_min_lr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_lr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_max_lr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_training\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_training_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[43m            \u001b[49m\u001b[43mearly_stop_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_early_stop_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[43m            \u001b[49m\u001b[43mupdate_attr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_attr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattr_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_attr_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_early_exit:\n\u001b[0;32m    126\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m _TunerExitException()\n",
      "File \u001b[1;32mc:\\Users\\gauta\\anaconda3\\envs\\keras-jax\\lib\\site-packages\\pytorch_lightning\\tuner\\lr_finder.py:302\u001b[0m, in \u001b[0;36m_lr_find\u001b[1;34m(trainer, model, min_lr, max_lr, num_training, mode, early_stop_threshold, update_attr, attr_name)\u001b[0m\n\u001b[0;32m    299\u001b[0m         log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLearning rate set to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    301\u001b[0m \u001b[38;5;66;03m# Restore initial state of model\u001b[39;00m\n\u001b[1;32m--> 302\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_checkpoint_connector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrestore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    303\u001b[0m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mremove_checkpoint(ckpt_path)\n\u001b[0;32m    304\u001b[0m trainer\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mrestarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# reset restarting flag as checkpoint restoring sets it to True\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\gauta\\anaconda3\\envs\\keras-jax\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\checkpoint_connector.py:240\u001b[0m, in \u001b[0;36m_CheckpointConnector.restore\u001b[1;34m(self, checkpoint_path)\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;66;03m# restore module states\u001b[39;00m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrestore_datamodule()\n\u001b[1;32m--> 240\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrestore_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;66;03m# restore callback states\u001b[39;00m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrestore_callbacks()\n",
      "File \u001b[1;32mc:\\Users\\gauta\\anaconda3\\envs\\keras-jax\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\checkpoint_connector.py:275\u001b[0m, in \u001b[0;36m_CheckpointConnector.restore_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    272\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_lightning_module_hook(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_load_checkpoint\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loaded_checkpoint)\n\u001b[0;32m    274\u001b[0m \u001b[38;5;66;03m# restore model state_dict\u001b[39;00m\n\u001b[1;32m--> 275\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model_state_dict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    276\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_loaded_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    277\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrict_loading\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    278\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\gauta\\anaconda3\\envs\\keras-jax\\lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:372\u001b[0m, in \u001b[0;36mStrategy.load_model_state_dict\u001b[1;34m(self, checkpoint, strict)\u001b[0m\n\u001b[0;32m    370\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_model_state_dict\u001b[39m(\u001b[38;5;28mself\u001b[39m, checkpoint: Mapping[\u001b[38;5;28mstr\u001b[39m, Any], strict: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    371\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 372\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstate_dict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\gauta\\anaconda3\\envs\\keras-jax\\lib\\site-packages\\torch\\nn\\modules\\module.py:2138\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[0;32m   2131\u001b[0m         out \u001b[38;5;241m=\u001b[39m hook(module, incompatible_keys)\n\u001b[0;32m   2132\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, (\n\u001b[0;32m   2133\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHooks registered with ``register_load_state_dict_post_hook`` are not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2134\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected to return new values, if incompatible_keys need to be modified,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2135\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mit should be done inplace.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2136\u001b[0m         )\n\u001b[1;32m-> 2138\u001b[0m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2139\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m load\n\u001b[0;32m   2141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m strict:\n",
      "File \u001b[1;32mc:\\Users\\gauta\\anaconda3\\envs\\keras-jax\\lib\\site-packages\\torch\\nn\\modules\\module.py:2126\u001b[0m, in \u001b[0;36mModule.load_state_dict.<locals>.load\u001b[1;34m(module, local_state_dict, prefix)\u001b[0m\n\u001b[0;32m   2124\u001b[0m         child_prefix \u001b[38;5;241m=\u001b[39m prefix \u001b[38;5;241m+\u001b[39m name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   2125\u001b[0m         child_state_dict \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m local_state_dict\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k\u001b[38;5;241m.\u001b[39mstartswith(child_prefix)}\n\u001b[1;32m-> 2126\u001b[0m         \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild_state_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild_prefix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2128\u001b[0m \u001b[38;5;66;03m# Note that the hook can modify missing_keys and unexpected_keys.\u001b[39;00m\n\u001b[0;32m   2129\u001b[0m incompatible_keys \u001b[38;5;241m=\u001b[39m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "File \u001b[1;32mc:\\Users\\gauta\\anaconda3\\envs\\keras-jax\\lib\\site-packages\\torch\\nn\\modules\\module.py:2126\u001b[0m, in \u001b[0;36mModule.load_state_dict.<locals>.load\u001b[1;34m(module, local_state_dict, prefix)\u001b[0m\n\u001b[0;32m   2124\u001b[0m         child_prefix \u001b[38;5;241m=\u001b[39m prefix \u001b[38;5;241m+\u001b[39m name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   2125\u001b[0m         child_state_dict \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m local_state_dict\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k\u001b[38;5;241m.\u001b[39mstartswith(child_prefix)}\n\u001b[1;32m-> 2126\u001b[0m         \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild_state_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild_prefix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2128\u001b[0m \u001b[38;5;66;03m# Note that the hook can modify missing_keys and unexpected_keys.\u001b[39;00m\n\u001b[0;32m   2129\u001b[0m incompatible_keys \u001b[38;5;241m=\u001b[39m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "File \u001b[1;32mc:\\Users\\gauta\\anaconda3\\envs\\keras-jax\\lib\\site-packages\\torch\\nn\\modules\\module.py:2120\u001b[0m, in \u001b[0;36mModule.load_state_dict.<locals>.load\u001b[1;34m(module, local_state_dict, prefix)\u001b[0m\n\u001b[0;32m   2118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m assign:\n\u001b[0;32m   2119\u001b[0m     local_metadata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124massign_to_params_buffers\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m assign\n\u001b[1;32m-> 2120\u001b[0m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_from_state_dict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_state_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_metadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_msgs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2122\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, child \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   2123\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m child \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\gauta\\anaconda3\\envs\\keras-jax\\lib\\site-packages\\torch\\nn\\modules\\module.py:2017\u001b[0m, in \u001b[0;36mModule._load_from_state_dict\u001b[1;34m(self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)\u001b[0m\n\u001b[0;32m   2014\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_param_lazy \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(param\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(input_param\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   2015\u001b[0m     input_param \u001b[38;5;241m=\u001b[39m input_param[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m-> 2017\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_param_lazy \u001b[38;5;129;01mand\u001b[39;00m \u001b[43minput_param\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m \u001b[38;5;241m!=\u001b[39m param\u001b[38;5;241m.\u001b[39mshape:\n\u001b[0;32m   2018\u001b[0m     \u001b[38;5;66;03m# local shape should match the one in checkpoint\u001b[39;00m\n\u001b[0;32m   2019\u001b[0m     error_msgs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msize mismatch for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m: copying a param with shape \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m from checkpoint, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   2020\u001b[0m                       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthe shape in current model is \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   2021\u001b[0m                       \u001b[38;5;241m.\u001b[39mformat(key, input_param\u001b[38;5;241m.\u001b[39mshape, param\u001b[38;5;241m.\u001b[39mshape))\n\u001b[0;32m   2022\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\gauta\\anaconda3\\envs\\keras-jax\\lib\\site-packages\\torch\\nn\\parameter.py:126\u001b[0m, in \u001b[0;36mUninitializedTensorMixin.shape\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshape\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 126\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    127\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124mt access the shape of an uninitialized parameter or buffer. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    128\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThis error usually happens in `load_state_dict` when trying to load \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    129\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124man uninitialized parameter into an initialized one. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    130\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCall `forward` to initialize the parameters before accessing their attributes.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Can't access the shape of an uninitialized parameter or buffer. This error usually happens in `load_state_dict` when trying to load an uninitialized parameter into an initialized one. Call `forward` to initialize the parameters before accessing their attributes."
     ]
    }
   ],
   "source": [
    "mod = Encode()\n",
    "trainer = pl.Trainer()\n",
    "tuner = Tuner(trainer)\n",
    "\n",
    "# Run learning rate finder\n",
    "lr_finder = tuner.lr_find(mod,train_dataloaders=tr_loader,val_dataloaders=val_loader)\n",
    "\n",
    "# Results can be found in\n",
    "print(lr_finder.results)\n",
    "\n",
    "# Plot with\n",
    "fig = lr_finder.plot(suggest=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lr_finder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mlr_finder\u001b[49m\u001b[38;5;241m.\u001b[39mresults)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Plot with\u001b[39;00m\n\u001b[0;32m      4\u001b[0m fig \u001b[38;5;241m=\u001b[39m lr_finder\u001b[38;5;241m.\u001b[39mplot(suggest\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'lr_finder' is not defined"
     ]
    }
   ],
   "source": [
    "print(lr_finder.results)\n",
    "\n",
    "# Plot with\n",
    "fig = lr_finder.plot(suggest=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras-jax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
